{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Read mnist dataset\n",
    "import sys, numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "plt.imshow(images[0].reshape(28,28))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "1 0\n",
      "2 4\n",
      "3 1\n",
      "4 9\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(labels[:5]):\n",
    "    print(i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple hidden nodes\n",
    "def relu(x):\n",
    "    return (x>0) * x #returns 1 if x>0 otherwise 0\n",
    "\n",
    "def reluderiv(output):\n",
    "    return (output>0) * 1 # returns 1 if output > 0 otherwise returns 0\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, inputs, hidden_layers, outputs, lr, verbose=False, \n",
    "                 weights_init_multiplier=0.2, \n",
    "                 weights_init_add=-0.1):\n",
    "        self.weights = []\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.weights_init_multiplier = weights_init_multiplier\n",
    "        self.weights_init_add = weights_init_add\n",
    "        previous_nodes = inputs     \n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.weights.append(self.get_weights(previous_nodes, self.hidden_layers[i]))\n",
    "            previous_nodes =  hidden_layers[i]\n",
    "        self.weights.append(self.get_weights(previous_nodes, outputs))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def get_weights(self, inputs, outputs):\n",
    "        # np.random.random returns random numbers in range 0..1\n",
    "        # if you use weights_init_multiplier = 0.2 and weights_init_add = -0.1 \n",
    "        # then you get numbers in range -0.1..0.1\n",
    "        return self.weights_init_multiplier * np.random.random((inputs, outputs)) + self.weights_init_add\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.layers = []\n",
    "        self.layers.append(input)\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.layers.append(relu(np.dot(self.layers[i],self.weights[i])))\n",
    "        \n",
    "        #last layer\n",
    "        self.layers.append(np.dot(self.layers[len(self.layers)-1],self.weights[len(self.layers)-1]))\n",
    "        \n",
    "        self.output = self.layers[len(self.layers)-1]\n",
    "        return self.output\n",
    "    \n",
    "    def calculate_error(self, correct_output):\n",
    "        self.errors = (correct_output - self.output) ** 2\n",
    "        self.error = np.sum(self.errors)\n",
    "            \n",
    "    def back_propagation(self, correct_output):\n",
    "        self.calculate_error(correct_output)        \n",
    "        weights_delta={}\n",
    "        \n",
    "        #Fix weights of last layer, no RELU in in last layer\n",
    "        delta = correct_output - self.output\n",
    "        weights_delta[len(self.weights)-1] = (self.lr * np.dot(self.layers[len(self.layers)-2].T, delta))\n",
    "                \n",
    "        # must calculate all deltas BEFORE updating weights\n",
    "        for i in range(len(self.weights)-2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i+1].T) * reluderiv(self.layers[i+1])\n",
    "            weights_delta[i] = self.lr * np.dot(self.layers[i].T, delta)\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += weights_delta[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:46 Train-Err:0.057 Train-Acc:0.993"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-7c95828bdf8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mcorrect_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_of_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# It's important that input is a matrix (#examples x 3 in this case) and not a vector!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mcorrect_cnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-bb0fb26e4d16>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m#last layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 350\n",
    "nn = NN(inputs=784, hidden_layers=(40,), outputs=10, lr=0.005, weights_init_multiplier=0.2, weights_init_add=-0.1)\n",
    "\n",
    "errors = []\n",
    "bs = 1\n",
    "current_batch = 0\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    correct_cnt = 0\n",
    "    error = 0\n",
    "    \n",
    "    total_batches = len(images)//bs\n",
    "    \n",
    "    for current_batch in range(total_batches):\n",
    "    \n",
    "        end_of_batch = current_batch * bs + bs\n",
    "        if current_batch+bs > len(images):\n",
    "            end_of_batch = len(images) + 1\n",
    "    \n",
    "        input = images[current_batch * bs:end_of_batch]\n",
    "        correct_output = labels[current_batch * bs:end_of_batch]\n",
    "        \n",
    "        nn.forward(input) # It's important that input is a matrix (#examples x 3 in this case) and not a vector!\n",
    "        for k in range(bs):\n",
    "            correct_cnt += (int(np.argmax(nn.output[k:k+1]) == np.argmax(correct_output[k:k+1])))\n",
    "        \n",
    "        nn.back_propagation(correct_output)\n",
    "        error += nn.error\n",
    "        \n",
    "    #print(f'Iteration: {iteration} Error: {error/float(len(images))} Train acc: {correct_cnt/float(len(images))}')\n",
    "    sys.stdout.write(\"\\r I:\"+str(iteration)+ \\\n",
    "                     \" Train-Err:\" + str(error/float(len(images)))[0:5] +\\\n",
    "                     \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " I:104 Test-Err:0.668 Test-Acc:0.8454"
     ]
    }
   ],
   "source": [
    "# Use the trained network in the test set\n",
    "correct_cnt = 0\n",
    "error = 0\n",
    "\n",
    "bs=1\n",
    "total_batches = len(test_images)//bs\n",
    "    \n",
    "for current_batch in range(total_batches):\n",
    "    \n",
    "    end_of_batch = current_batch * bs + bs\n",
    "    if current_batch+bs > len(test_images):\n",
    "        end_of_batch = len(test_images) + 1\n",
    "    \n",
    "    input = test_images[current_batch * bs:end_of_batch + 1]\n",
    "    correct_output = test_labels[current_batch * bs:end_of_batch + 1]\n",
    "        \n",
    "    nn.forward(input) # It's important that input is a matrix (#examples x 3 in this case) and not a vector!\n",
    "    for k in range(bs):\n",
    "        correct_cnt += (int(np.argmax(nn.output[k:k+1]) == np.argmax(correct_output[k:k+1])))\n",
    "        \n",
    "    nn.calculate_error(correct_output)\n",
    "    error += nn.error\n",
    "        \n",
    "    #print(f'Iteration: {iteration} Error: {error/float(len(images))} Train acc: {correct_cnt/float(len(images))}')\n",
    "sys.stdout.write(\"\\r I:\"+str(iteration)+ \\\n",
    "                 \" Test-Err:\" + str(error/float(len(test_images)))[0:5] +\\\n",
    "                 \" Test-Acc:\" + str(correct_cnt/float(len(test_images))))\n",
    "errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple hidden nodes and dropout\n",
    "def relu(x):\n",
    "    return (x>0) * x #returns 1 if x>0 otherwise 0\n",
    "\n",
    "def reluderiv(output):\n",
    "    return (output>0) * 1 # returns 1 if output > 0 otherwise returns 0\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, inputs, hidden_layers, outputs, lr, verbose=False, dropouts=None,\n",
    "                 weights_init_multiplier=0.2, \n",
    "                 weights_init_add=-0.1):\n",
    "        self.weights = []\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.weights_init_multiplier = weights_init_multiplier\n",
    "        self.weights_init_add = weights_init_add\n",
    "        self.dropouts = dropouts\n",
    "        \n",
    "        previous_nodes = inputs     \n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.weights.append(self.get_weights(previous_nodes, self.hidden_layers[i]))\n",
    "            previous_nodes =  hidden_layers[i]\n",
    "        self.weights.append(self.get_weights(previous_nodes, outputs))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def get_weights(self, inputs, outputs):\n",
    "        # np.random.random returns random numbers in range 0..1\n",
    "        # if you use weights_init_multiplier = 0.2 and weights_init_add = -0.1 \n",
    "        # then you get numbers in range -0.1..0.1\n",
    "        return self.weights_init_multiplier * np.random.random((inputs, outputs)) + self.weights_init_add\n",
    "    \n",
    "    def forward(self, input, is_training):\n",
    "        self.layers = []\n",
    "        self.dropout_masks = []\n",
    "        self.layers.append(input)\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.layers.append(relu(np.dot(self.layers[i],self.weights[i])))\n",
    "            if not self.dropouts is None:\n",
    "                dropout_ratio = self.dropouts[len(self.layers)-2]\n",
    "                if dropout_ratio>0:\n",
    "                    dropout_mask = np.random.random((self.hidden_layers[i],))\n",
    "                    # Compensate for the inactive nodes by multiplying remaining nodes with a number\n",
    "                    dropout_mask = [1/(1-dropout_ratio) if x>dropout_ratio else 0 for x in dropout_mask]                   \n",
    "                    self.layers[len(self.layers)-1] *= dropout_mask\n",
    "                    self.dropout_masks.append(dropout_mask)\n",
    "                else:\n",
    "                    self.dropout_masks.append(None)\n",
    "        \n",
    "        #last layer\n",
    "        self.layers.append(np.dot(self.layers[len(self.layers)-1],self.weights[len(self.layers)-1]))\n",
    "        \n",
    "        self.output = self.layers[len(self.layers)-1]\n",
    "        return self.output\n",
    "    \n",
    "    def calculate_error(self, correct_output):\n",
    "        self.errors = (correct_output - self.output) ** 2\n",
    "        self.error = np.sum(self.errors)\n",
    "            \n",
    "    def back_propagation(self, correct_output):\n",
    "        self.calculate_error(correct_output)        \n",
    "        weights_delta={}\n",
    "        \n",
    "        #Fix weights of last layer, no RELU in in last layer\n",
    "        delta = correct_output - self.output\n",
    "        weights_delta[len(self.weights)-1] = (self.lr * np.dot(self.layers[len(self.layers)-2].T, delta))\n",
    "                \n",
    "        # must calculate all deltas BEFORE updating weights\n",
    "        for i in range(len(self.weights)-2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i+1].T) * reluderiv(self.layers[i+1])\n",
    "            if not self.dropout_masks[i] is None:\n",
    "                delta *= self.dropout_masks[i]\n",
    "            weights_delta[i] = self.lr * np.dot(self.layers[i].T, delta)\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += weights_delta[i]\n",
    "            \n",
    "    def describe(self):\n",
    "        print(f'Inputs: {self.weights[0].shape[0]}')\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            print(f'Weights {i}: {self.weights[i].shape}')\n",
    "            print(f'Hidden layer {i}: {self.hidden_layers[i]}')\n",
    "        \n",
    "        print(f'Weights {len(self.weights)-1}:{self.weights[len(self.weights)-1].shape}')\n",
    "        print(f'Ouputs: {self.weights[len(self.weights)-1].shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 784\n",
      "Weights 0: (784, 100)\n",
      "Hidden layer 0: 100\n",
      "Weights 1:(100, 10)\n",
      "Ouputs: 10\n",
      " I:349 Train-Err:0.153 Train-Acc:0.981 Test-Acc:0.8289"
     ]
    }
   ],
   "source": [
    "epochs = 350\n",
    "nn = NN(inputs=784, hidden_layers=(100,), outputs=10, lr=0.005, \n",
    "        weights_init_multiplier=0.2, weights_init_add=-0.1, dropouts=(0.5,))\n",
    "nn.describe()\n",
    "\n",
    "\n",
    "errors = []\n",
    "bs = 1\n",
    "current_batch = 0\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    correct_cnt = 0\n",
    "    error = 0\n",
    "    \n",
    "    total_batches = len(images)//bs\n",
    "    \n",
    "    for current_batch in range(total_batches):\n",
    "    \n",
    "        end_of_batch = current_batch * bs + bs\n",
    "        if current_batch+bs > len(images):\n",
    "            end_of_batch = len(images) + 1\n",
    "    \n",
    "        input = images[current_batch * bs:end_of_batch]\n",
    "        correct_output = labels[current_batch * bs:end_of_batch]\n",
    "        \n",
    "        # Dropout is used only while training\n",
    "        nn.forward(input, is_training=True) \n",
    "        for k in range(bs):\n",
    "            correct_cnt += (int(np.argmax(nn.output[k:k+1]) == np.argmax(correct_output[k:k+1])))\n",
    "        \n",
    "        nn.back_propagation(correct_output)\n",
    "        error += nn.error\n",
    "    \n",
    "    if (iteration % 10) == 0 or (iteration==epochs-1):\n",
    "        \n",
    "        correct_cnt_test = 0\n",
    "        # Evaluate on the test set\n",
    "        for i in range(len(test_images)):\n",
    "            input = test_images[i:i+1]\n",
    "            correct_output = test_labels[i:i+1]\n",
    "        \n",
    "            nn.forward(input, is_training=False)\n",
    "            correct_cnt_test += (int(np.argmax(nn.output[0]) == np.argmax(correct_output)))\n",
    "            \n",
    "    #print(f'Iteration: {iteration} Error: {error/float(len(images))} Train acc: {correct_cnt/float(len(images))}')\n",
    "        sys.stdout.write(\"\\r I:\"+str(iteration)+ \\\n",
    "                         \" Train-Err:\" + str(error/float(len(images)))[0:5] +\\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/float(len(images))) +\\\n",
    "                         \" Test-Acc:\" + str(correct_cnt_test/float(len(test_images))))\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple hidden nodes and dropout and activation functions\n",
    "def relu(x):\n",
    "    return (x>0) * x #returns 1 if x>0 otherwise 0\n",
    "\n",
    "def reluderiv(output):\n",
    "    return (output>0) * 1 # returns 1 if output > 0 otherwise returns 0\n",
    "\n",
    "def tanh(output):\n",
    "    return np.tanh(output)\n",
    "\n",
    "def tanhderiv(output):\n",
    "    return 1-(output**2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp/np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, inputs, hidden_layers, outputs, lr, verbose=False, dropouts=None,\n",
    "                 weights_init_multiplier=0.2, weights_init_add=-0.1,\n",
    "                 intermediate_activation_function='relu', last_activation_function=''):\n",
    "        self.weights = []\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.weights_init_multiplier = weights_init_multiplier\n",
    "        self.weights_init_add = weights_init_add\n",
    "        self.dropouts = dropouts\n",
    "        \n",
    "        self.intermediate_activation_function=relu\n",
    "        self.intermediate_activation_function_deriv=reluderiv\n",
    "        if intermediate_activation_function=='tanh':\n",
    "            self.intermediate_activation_function=tanh\n",
    "            self.intermediate_activation_function_deriv=tanhderiv\n",
    "                        \n",
    "        self.last_activation_function=None\n",
    "        if last_activation_function=='softmax':\n",
    "            self.last_activation_function=softmax\n",
    "                \n",
    "        previous_nodes = inputs     \n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            if isinstance(weights_init_multiplier, float):\n",
    "                mult = weights_init_multiplier\n",
    "            else: #tuple\n",
    "                mult = weights_init_multiplier[i]\n",
    "                \n",
    "            if isinstance(weights_init_add, float):\n",
    "                add = weights_init_add\n",
    "            else: #tuple\n",
    "                add = weights_init_add[i]\n",
    "                \n",
    "            self.weights.append(self.get_weights(previous_nodes, self.hidden_layers[i], mult, add))\n",
    "            previous_nodes =  hidden_layers[i]\n",
    "        \n",
    "        if isinstance(weights_init_multiplier, float):\n",
    "            mult = weights_init_multiplier\n",
    "        else: #tuple\n",
    "            mult = weights_init_multiplier[len(self.hidden_layers)]\n",
    "                \n",
    "        if isinstance(weights_init_add, float):\n",
    "            add = weights_init_add\n",
    "        else: #tuple\n",
    "            add = weights_init_add[len(self.hidden_layers)]\n",
    "        self.weights.append(self.get_weights(previous_nodes, outputs, mult, add))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def get_weights(self, inputs, outputs, mult, add):\n",
    "        # np.random.random returns random numbers in range 0..1\n",
    "        # if you use weights_init_multiplier = 0.2 and weights_init_add = -0.1 \n",
    "        # then you get numbers in range -0.1..0.1\n",
    "        return mult * np.random.random((inputs, outputs)) + add\n",
    "    \n",
    "    def forward(self, input, is_training):\n",
    "        self.layers = []\n",
    "        self.dropout_masks = []\n",
    "        self.layers.append(input)\n",
    "        \n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.layers.append(self.intermediate_activation_function(np.dot(self.layers[i],self.weights[i])))\n",
    "            if not self.dropouts is None:\n",
    "                dropout_ratio = self.dropouts[len(self.layers)-2]\n",
    "                if dropout_ratio>0:\n",
    "                    dropout_mask = np.random.random((self.hidden_layers[i],))\n",
    "                    # Compensate for the inactive nodes by multiplying remaining nodes with a number\n",
    "                    dropout_mask = [1/(1-dropout_ratio) if x>dropout_ratio else 0 for x in dropout_mask]                   \n",
    "                    self.layers[len(self.layers)-1] *= dropout_mask\n",
    "                    self.dropout_masks.append(dropout_mask)\n",
    "                else:\n",
    "                    self.dropout_masks.append(None)\n",
    "        \n",
    "        #last layer, in the example softmax was not used while predicting on the test set (why???)\n",
    "        if (self.last_activation_function is None) or (is_training==False):\n",
    "            self.layers.append(np.dot(self.layers[len(self.layers)-1],self.weights[len(self.layers)-1]))\n",
    "        else:\n",
    "            self.layers.append(self.last_activation_function(np.dot(self.layers[len(self.layers)-1],self.weights[len(self.layers)-1])))\n",
    "        \n",
    "        self.output = self.layers[len(self.layers)-1]\n",
    "        return self.output\n",
    "    \n",
    "    def calculate_error(self, correct_output):\n",
    "        self.errors = (correct_output - self.output) ** 2\n",
    "        self.error = np.sum(self.errors)\n",
    "            \n",
    "    def back_propagation(self, correct_output):\n",
    "        self.calculate_error(correct_output)        \n",
    "        weights_delta={}\n",
    "        \n",
    "        #Fix weights of last layer\n",
    "        # For Softmax it's important to use / (input.shape[0]**2) !!!!\n",
    "        # Probably this is because softmax outputs very small values for all other values\n",
    "        #Example: [0.00341138 0.01589889 0.00174605 0.02112253 0.0406359  0.05197833, \n",
    "        # 0.00452068 0.01234159 0.03014057 0.81820407]\n",
    "        delta = (correct_output - self.output) / (input.shape[0]**2)\n",
    "        \n",
    "        weights_delta[len(self.weights)-1] = (self.lr * np.dot(self.layers[len(self.layers)-2].T, delta))\n",
    "                \n",
    "        # must calculate all deltas BEFORE updating weights\n",
    "        for i in range(len(self.weights)-2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i+1].T) * self.intermediate_activation_function_deriv(self.layers[i+1])\n",
    "            if not self.dropout_masks[i] is None:\n",
    "                delta *= self.dropout_masks[i]\n",
    "            weights_delta[i] = self.lr * np.dot(self.layers[i].T, delta)\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += weights_delta[i]\n",
    "            \n",
    "    def describe(self):\n",
    "        print(f'Inputs: {self.weights[0].shape[0]}')\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            print(f'Weights {i}: {self.weights[i].shape}')\n",
    "            print(f'Hidden layer {i}: {self.hidden_layers[i]}')\n",
    "            print(f'Activation function {i}: {self.intermediate_activation_function.__name__}')\n",
    "        \n",
    "        print(f'Weights {len(self.weights)-1}:{self.weights[len(self.weights)-1].shape}')\n",
    "        if not self.last_activation_function is None:\n",
    "            print(f'Last activation function: {self.last_activation_function.__name__}')\n",
    "        print(f'Ouputs: {self.weights[len(self.weights)-1].shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 784\n",
      "Weights 0: (784, 100)\n",
      "Hidden layer 0: 100\n",
      "Activation function 0: tanh\n",
      "Weights 1:(100, 10)\n",
      "Last activation function: softmax\n",
      "Ouputs: 10\n",
      " I:60 Train-Err:0.405 Train-Acc:0.828 Test-Acc:0.7825"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-963903a174fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mcorrect_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mcorrect_cnt_test\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-168-3df7ba7b83fb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, is_training)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m#last layer, in the example softmax was not used while predicting on the test set (why???)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_activation_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_activation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 350\n",
    "#nn = NN(inputs=784, hidden_layers=(100,), outputs=10, lr=0.005, \n",
    "#        weights_init_multiplier=0.2, weights_init_add=-0.1, dropouts=(0.5,))\n",
    "\n",
    "nn = NN(inputs=784, hidden_layers=(100,), outputs=10, lr=1, \n",
    "        weights_init_multiplier=(0.02,0.2), weights_init_add=(-0.01,-0.1), dropouts=(0.5,),\n",
    "        intermediate_activation_function='tanh', last_activation_function='softmax')\n",
    "\n",
    "nn.describe()\n",
    "\n",
    "\n",
    "errors = []\n",
    "bs = 100\n",
    "current_batch = 0\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    correct_cnt = 0\n",
    "    error = 0\n",
    "    \n",
    "    total_batches = len(images)//bs\n",
    "    \n",
    "    for current_batch in range(total_batches):\n",
    "    \n",
    "        end_of_batch = current_batch * bs + bs\n",
    "        if current_batch+bs > len(images):\n",
    "            end_of_batch = len(images) + 1\n",
    "    \n",
    "        input = images[current_batch * bs:end_of_batch]\n",
    "        correct_output = labels[current_batch * bs:end_of_batch]\n",
    "        \n",
    "        # Dropout is used only while training\n",
    "        nn.forward(input, is_training=True) \n",
    "        for k in range(bs):\n",
    "            correct_cnt += (int(np.argmax(nn.output[k:k+1]) == np.argmax(correct_output[k:k+1])))\n",
    "        \n",
    "        nn.back_propagation(correct_output)\n",
    "        error += nn.error\n",
    "    \n",
    "    if (iteration % 10) == 0 or (iteration==epochs-1):\n",
    "        \n",
    "        correct_cnt_test = 0\n",
    "        # Evaluate on the test set\n",
    "        for i in range(len(test_images)):\n",
    "            input = test_images[i:i+1]\n",
    "            correct_output = test_labels[i:i+1]\n",
    "        \n",
    "            nn.forward(input, is_training=False)\n",
    "            correct_cnt_test += (int(np.argmax(nn.output[0]) == np.argmax(correct_output)))\n",
    "            \n",
    "    #print(f'Iteration: {iteration} Error: {error/float(len(images))} Train acc: {correct_cnt/float(len(images))}')\n",
    "        sys.stdout.write(\"\\r I:\"+str(iteration)+ \\\n",
    "                         \" Train-Err:\" + str(error/float(len(images)))[0:5] +\\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/float(len(images))) +\\\n",
    "                         \" Test-Acc:\" + str(correct_cnt_test/float(len(test_images))))\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 3.6",
   "language": "python",
   "name": "tensorflow36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
