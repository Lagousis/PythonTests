{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter12%20-%20Intro%20to%20Recurrence%20-%20Predicting%20the%20Next%20Word.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download & Preprocess the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reviews.txt and labels.txt from here: https://github.com/udacity/deep-learning/tree/master/sentiment-network\n",
    "\n",
    "def pretty_print_review_and_label(i):\n",
    "   print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('data/reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('data/labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "\n",
    "# Preprocess dataset:\n",
    "\n",
    "import sys\n",
    "\n",
    "f = open('data/reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('data/labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Split reviews and get all words. Tokens is a list of reviews, but each review is a list of words\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
    "\n",
    "# Get distinct words\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Assign an integer to each word\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "#input_dataset is a list of reviews but each word is replaced with the corresponding index\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "\n",
    "#labels\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a NN to predict reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:1 Progress:95.99% Training Accuracy:0.8670208333333334%Test Accuracy:0.851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1) \n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "alpha, iterations = (0.01, 2) \n",
    "hidden_size = 100 \n",
    "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1 \n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1 \n",
    "correct,total = (0,0) \n",
    "\n",
    "for iter in range(iterations): \n",
    "    # train on first 24,000 \n",
    "    for i in range(len(input_dataset)-1000): \n",
    "        x,y = (input_dataset[i],target_dataset[i]) \n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) #embed + sigmoid \n",
    "        layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) # linear + softmax  #Stavros: sigmoid\n",
    "        \n",
    "        layer_2_delta = layer_2 - y # compare pred with truth\n",
    "\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) #backprop \n",
    "        \n",
    "        weights_0_1[x] -= layer_1_delta * alpha \n",
    "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha \n",
    "        \n",
    "        if(np.abs(layer_2_delta) < 0.5): \n",
    "            correct += 1 \n",
    "        total += 1 \n",
    "        \n",
    "        if(i % 10 == 9): \n",
    "            progress = str(i/float(len(input_dataset))) \n",
    "            sys.stdout.write('\\rIter:'+str(iter) +\n",
    "                             ' Progress:' + progress[2:4] +\n",
    "                             '.' +progress[4:6] +\n",
    "                             '% Training Accuracy:' +\n",
    "                             str(correct/float(total)) + '%') \n",
    "        \n",
    "correct,total = (0,0) \n",
    "for i in range(len(input_dataset)-1000,len(input_dataset)): \n",
    "    x = input_dataset[i] \n",
    "    y = target_dataset[i] \n",
    "    \n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) \n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) \n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5): \n",
    "        correct += 1 \n",
    "        \n",
    "    total += 1 \n",
    "    \n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Surprising Power of Averaged Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no comment  stupid movie  acting average or worse . . . screenplay  no sense at all . . . skip it   \\n',\n",
       " 'i wouldn  t rent this one even on dollar rental night .  \\n',\n",
       " 'this video is     retarded . besides the brain cell killing acting and plot  it  s way too long . don  t waste your money at the video store . i actually was mad that i sat through this garbage and spent money on it . just absolutely awful .  \\n']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#Original\n",
    "#norms = np.sum(weights_0_1 * weights_0_1,axis=1)\n",
    "#norms.resize(norms.shape[0],1)\n",
    "#normed_weights = weights_0_1 * norms\n",
    "\n",
    "#Stavros\n",
    "#L1 normalization\n",
    "#norms = np.sum(weights_0_1 ** 2 ,axis=1, keepdims=1)\n",
    "#normed_weights = weights_0_1 / norms\n",
    "\n",
    "#L2 normalization\n",
    "norms = np.sqrt(np.sum(weights_0_1 ** 2 ,axis=1, keepdims=1))\n",
    "normed_weights = weights_0_1 / norms\n",
    "\n",
    "\n",
    "def make_sent_vect(words):\n",
    "    #filter(lambda x:x in word2index,words) -> Get only words that exist in word2index\n",
    "    #map(lambda x:word2index[x] -> get index of each word\n",
    "    indices = list(map(lambda x:word2index[x],filter(lambda x:x in word2index,words)))\n",
    "    #take the embeddings of each word and find the mean (normalized)\n",
    "    return np.mean(normed_weights[indices],axis=0)\n",
    "\n",
    "#Create an array that has the average embeddings for the words of each review\n",
    "reviews2vectors = list()\n",
    "for review in tokens: # tokenized reviews\n",
    "    reviews2vectors.append(make_sent_vect(review))\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    #Find the vector for this review\n",
    "    v = make_sent_vect(review)\n",
    "    most_similar = list()\n",
    "    \n",
    "    scores = Counter()\n",
    "    #if two vectors are similar then their product (element by element and then sum) will be bigger\n",
    "    for i,val in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    \n",
    "    for idx,score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx])\n",
    "    return most_similar\n",
    "\n",
    "def most_similar_reviews_stavros(review):\n",
    "    # Faster version, make matrix multiplication\n",
    "    #Find the vector for this review\n",
    "    v = make_sent_vect(review)\n",
    "    v.resize(v.shape[0],1)    \n",
    "    review_scores = np.dot(reviews2vectors, v)\n",
    "    best3 = np.argsort(review_scores, axis=0)[::-1][:3]\n",
    "    most_similar = list()\n",
    "    for r in best3:\n",
    "        most_similar.append(raw_reviews[int(r)])\n",
    "    return most_similar\n",
    "\n",
    "most_similar_reviews(['boring','awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.4 ms ± 272 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit most_similar_reviews(['boring','awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no comment  stupid movie  acting average or worse . . . screenplay  no sense at all . . . skip it   \\n',\n",
       " 'i wouldn  t rent this one even on dollar rental night .  \\n',\n",
       " 'this video is     retarded . besides the brain cell killing acting and plot  it  s way too long . don  t waste your money at the video store . i actually was mad that i sat through this garbage and spent money on it . just absolutely awful .  \\n']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_reviews_stavros(['boring','awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1 ms ± 49.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit most_similar_reviews_stavros(['boring','awful'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization\n",
    "\n",
    "The point of normalization is to make variables comparable to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34773861]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34773861],\n",
       "       [0.4001928 ],\n",
       "       [0.31854884],\n",
       "       [0.42441024],\n",
       "       [0.37752401]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First you take each row and you sum its squared weights\n",
    "np.sum(list(map(lambda x:x**2, weights_0_1[0:5,:])), axis=1)\n",
    "# if you add keepdims=1 then there is no need for resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34773861, 0.4001928 , 0.31854884, 0.42441024, 0.37752401])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#norms.resize(norms.shape[0],1) creates a column matrix (instead of a row matrix)\n",
    "norms[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00594589,  0.01537957, -0.03479716, ...,  0.00491532,\n",
       "        -0.03442329,  0.00828448],\n",
       "       [-0.01425374,  0.00224013,  0.0308414 , ..., -0.02168088,\n",
       "         0.00961813,  0.03623475],\n",
       "       [ 0.02865229,  0.00362072,  0.02647069, ..., -0.02681901,\n",
       "         0.03079221, -0.02025997],\n",
       "       ...,\n",
       "       [-0.04057025, -0.03256796, -0.005784  , ..., -0.01269812,\n",
       "        -0.00762553, -0.01685829],\n",
       "       [-0.03169719, -0.02640913, -0.02081986, ..., -0.02510512,\n",
       "        -0.00972922,  0.03498913],\n",
       "       [ 0.01241721, -0.01713725,  0.00996594, ...,  0.00147966,\n",
       "         0.00266909, -0.03177905]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normed_weights is the weights multiplied (each column) with the normed_weights\n",
    "normed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58969366],\n",
       "       [0.63260794],\n",
       "       [0.56440131],\n",
       "       ...,\n",
       "       [0.63350113],\n",
       "       [0.60857029],\n",
       "       [0.57917452]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = np.sqrt(np.sum(weights_0_1 ** 2 ,axis=1, keepdims=1))\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1020982 , -0.06021745, -0.00360221, -0.09817285, -0.05933014,\n",
       "       -0.01533265, -0.09012846, -0.06562662, -0.07816698, -0.01903082,\n",
       "       -0.07730621, -0.05130723, -0.09841031, -0.09841245, -0.05194574,\n",
       "       -0.01967773, -0.08983353, -0.03819163, -0.05328994,  0.01045728,\n",
       "       -0.10898989, -0.05697306, -0.01559359, -0.02538482, -0.04711993,\n",
       "       -0.06467783, -0.08864671, -0.03766169, -0.12726829, -0.04923117,\n",
       "       -0.08250164, -0.09214022, -0.02291921, -0.00836783,  0.0204451 ,\n",
       "       -0.08115479, -0.09363602, -0.09149977, -0.04685668, -0.092641  ,\n",
       "       -0.0653623 , -0.13240103, -0.06144531, -0.08910579, -0.07003325,\n",
       "       -0.0757983 , -0.0002148 , -0.11973187, -0.04660859, -0.04476614,\n",
       "       -0.04897268, -0.02885444, -0.05793279,  0.00196865, -0.02640572,\n",
       "       -0.00641995, -0.0517071 , -0.09895301, -0.06323607, -0.0128212 ,\n",
       "       -0.02656843, -0.03967399, -0.10007042, -0.02573938, -0.10169405,\n",
       "       -0.05913026, -0.01960393, -0.05321616, -0.02065232, -0.06973726,\n",
       "       -0.07256231, -0.08862624, -0.03338183, -0.06846108, -0.02796509,\n",
       "       -0.08700124, -0.02432658, -0.08165104, -0.05699242, -0.02811174,\n",
       "       -0.04854562, -0.11194833, -0.0324759 , -0.06726829, -0.06630207,\n",
       "       -0.05194576, -0.01560914, -0.03312291, -0.02421268, -0.00351684,\n",
       "       -0.06387716, -0.12649499, -0.02772497, -0.07588038, -0.03310465,\n",
       "       -0.06269599, -0.07759431, -0.04256525, -0.08772375, -0.05141694])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=['this','is','a','good','movie']\n",
    "make_sent_vect(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11457499442673733"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=['this','is','a','good','movie']\n",
    "this_review = make_sent_vect(words)\n",
    "reviews2vectors[0].dot(this_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11457499, 0.11035165, 0.07141358, ..., 0.10392294, 0.09140595,\n",
       "       0.12656131])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(reviews2vectors, this_review.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews2vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11656979, -0.03717042,  0.04483297,  0.07379027,  0.11533581,\n",
       "        -0.0532174 , -0.02377336, -0.02717042, -0.04595508,  0.04428797,\n",
       "         0.12167114,  0.06485789,  0.11951096,  0.0059348 , -0.11142395,\n",
       "         0.09682731,  0.10801687,  0.03978317,  0.07677372,  0.0285973 ,\n",
       "         0.02481736, -0.0769564 , -0.06408165, -0.05550242, -0.04635753,\n",
       "        -0.12897093,  0.0550261 , -0.01955263, -0.0079733 , -0.09731283,\n",
       "         0.16769118, -0.08955328,  0.08544972, -0.09872328, -0.15019725,\n",
       "         0.06836418, -0.0620496 ,  0.01191898, -0.0757797 , -0.01206828,\n",
       "        -0.1270169 ,  0.06085481,  0.00281248, -0.10286752, -0.06072698,\n",
       "         0.04018963,  0.05444234,  0.2092833 , -0.16261433, -0.09929475,\n",
       "         0.08291038,  0.11714358, -0.02383672, -0.07581927,  0.00086753,\n",
       "        -0.06742831, -0.20914505, -0.00261467, -0.10479984,  0.08466371,\n",
       "         0.06961278, -0.16640474,  0.07871799,  0.06555903,  0.07629995,\n",
       "        -0.14045947, -0.01900039, -0.1048364 ,  0.09986592,  0.04069377,\n",
       "        -0.13492536, -0.21740712, -0.01363568,  0.00096377,  0.17821461,\n",
       "         0.23618961,  0.10471629,  0.06854561,  0.22363594, -0.05261314,\n",
       "        -0.04559533, -0.07395754, -0.09887508, -0.0471872 , -0.00838268,\n",
       "         0.17661798, -0.15659351,  0.13273589,  0.05089974, -0.10866781,\n",
       "        -0.08772248, -0.06395756,  0.038766  , -0.12068632, -0.02184142,\n",
       "        -0.01134189, -0.01471313,  0.14114438, -0.13545875, -0.08913907]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=['boring','awful']\n",
    "this_review = make_sent_vect(words)\n",
    "this_review.resize(this_review.shape[0],1)\n",
    "this_review.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no comment  stupid movie  acting average or worse . . . screenplay  no sense at all . . . skip it   \n",
      "\n",
      "i wouldn  t rent this one even on dollar rental night .  \n",
      "\n",
      "this video is     retarded . besides the brain cell killing acting and plot  it  s way too long . don  t waste your money at the video store . i actually was mad that i sat through this garbage and spent money on it . just absolutely awful .  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_scores = np.dot(reviews2vectors, this_review)\n",
    "best3 = np.argsort(review_scores, axis=0)[::-1][:3]\n",
    "for r in best3:\n",
    "    print(raw_reviews[int(r)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no comment  stupid movie  acting average or worse . . . screenplay  no sense at all . . . skip it   \\n',\n",
       " 'i wouldn  t rent this one even on dollar rental night .  \\n',\n",
       " 'this video is     retarded . besides the brain cell killing acting and plot  it  s way too long . don  t waste your money at the video store . i actually was mad that i sat through this garbage and spent money on it . just absolutely awful .  \\n']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raw_reviews[int(i)] for i in best3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.36046042]), array([0.35627966]), array([0.29942664])]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[review_scores[int(i)] for i in best3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
     ]
    }
   ],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('data/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "print(tokens[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 Mary moved to the bathroom.\\n',\n",
       " '2 John went to the hallway.\\n',\n",
       " '3 Where is Mary? \\tbathroom\\t1\\n']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 56, 28]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2indices(['where','is','mary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "#Embedding size = 10, that means that we wiil create a vector of 10 numbers for each word\n",
    "embed_size = 20\n",
    "\n",
    "# word embeddings\n",
    "embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1\n",
    "\n",
    "# embedding -> embedding (initially the identity matrix)\n",
    "recurrent = np.eye(embed_size)\n",
    "\n",
    "# sentence embedding for empty sentence\n",
    "start = np.zeros(embed_size)\n",
    "\n",
    "# embedding -> output weights\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "\n",
    "# one hot lookups (for loss function)\n",
    "one_hot = np.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent, more_info=False):\n",
    "    \n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # forward propagate\n",
    "    #preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "        \n",
    "        if more_info:\n",
    "            print(f'Word: {vocab[sent[target_i]]}')\n",
    "            \n",
    "        layer = {}\n",
    "\n",
    "        # try to predict the next term\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "\n",
    "        # generate the next hidden state\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "\n",
    "    return layers, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
    "    layers,loss = predict(sent) \n",
    "\n",
    "    # back propagate\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "\n",
    "        if(layer_idx > 0):  # if not the first layer\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "\n",
    "            # if the last layer - don't pull from a later one becasue it doesn't exist\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else: # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Update with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:81.90689914965836\n",
      "Perplexity:81.54874909835365\n",
      "Perplexity:81.03952457929005\n",
      "Perplexity:80.14267876135821\n",
      "Perplexity:78.31074152740617\n",
      "Perplexity:73.76327560814543\n",
      "Perplexity:56.13299305501793\n",
      "Perplexity:28.223751127519517\n",
      "Perplexity:20.12824121277819\n",
      "Perplexity:18.539255904287455\n",
      "Perplexity:17.076770581229606\n",
      "Perplexity:14.986328527053107\n",
      "Perplexity:11.994811346474723\n",
      "Perplexity:8.85898998314854\n",
      "Perplexity:7.141389277973937\n",
      "Perplexity:6.202298539780477\n",
      "Perplexity:5.538030602558332\n",
      "Perplexity:5.123305561689193\n",
      "Perplexity:4.843939793340768\n",
      "Perplexity:4.647976745666142\n",
      "Perplexity:4.517704396878363\n",
      "Perplexity:4.445064840498462\n",
      "Perplexity:4.39488698107356\n",
      "Perplexity:4.333269320320349\n",
      "Perplexity:4.255353662992695\n",
      "Perplexity:4.165874552131964\n",
      "Perplexity:4.071358453102025\n",
      "Perplexity:3.9723242218998362\n",
      "Perplexity:3.896626409647608\n",
      "Perplexity:3.8757768816207445\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter % len(tokens)][1:])\n",
    "\n",
    "    layers,loss = predict(sent) \n",
    "\n",
    "    # back propagate\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "\n",
    "        if(layer_idx > 0):\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "\n",
    "            # if the last layer - don't pull from a \n",
    "            # later one becasue it doesn't exist\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else:\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "\n",
    "    # update weights\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    for layer_idx,layer in enumerate(layers[1:]):\n",
    "        \n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sent))\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "    if(iter % 1000 == 0):\n",
    "        print(\"Perplexity:\" + str(np.exp(loss/len(sent))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution and Output Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: sandra\n",
      "Word: moved\n",
      "Word: to\n",
      "Word: the\n",
      "Word: garden.\n",
      "['sandra', 'moved', 'to', 'the', 'garden.']\n",
      "Prev Input:sandra      True:moved          Pred:is\n",
      "Prev Input:moved       True:to             Pred:to\n",
      "Prev Input:to          True:the            Pred:the\n",
      "Prev Input:the         True:garden.        Pred:bedroom.\n"
     ]
    }
   ],
   "source": [
    "sent_index = 4\n",
    "\n",
    "l,_ = predict(words2indices(tokens[sent_index]), True)\n",
    "\n",
    "print(tokens[sent_index])\n",
    "\n",
    "for i,each_layer in enumerate(l[1:-1]):\n",
    "    input = tokens[sent_index][i]\n",
    "    true = tokens[sent_index][i+1]\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\n",
    "          \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moved', 'to', 'the', 'bathroom.']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words2indices(tokens[2 % len(tokens)][1:])\n",
    "tokens[0 % len(tokens)][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24618294,  0.25083996, -0.05009852, -0.21215657, -0.21104805,\n",
       "        -0.30466606, -0.5333955 , -0.23242285,  0.50067959, -0.06018655,\n",
       "         0.08897665, -0.06850264,  0.00411577,  0.6148313 , -0.28214099,\n",
       "        -0.4283181 , -0.30010674, -0.08342092,  0.06842135, -0.06700439],\n",
       "       [ 0.02731105,  0.0433735 , -0.01456817,  0.02196471,  0.04397357,\n",
       "         0.04522468, -0.04135362, -0.04734088, -0.04050605,  0.04174311,\n",
       "        -0.03779046, -0.00585994,  0.04462329, -0.00376893,  0.02280841,\n",
       "        -0.01337451,  0.02232304,  0.03404385, -0.04998701,  0.02538059]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recurrent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 82)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36 env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
