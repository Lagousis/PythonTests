{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from transformers import CTRLLMHeadModel, CTRLTokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer\n",
    "\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig)), ())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'ctrl': (CTRLLMHeadModel, CTRLTokenizer),\n",
    "    'openai-gpt': (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'xlnet': (XLNetLMHeadModel, XLNetTokenizer),\n",
    "    'transfo-xl': (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "    'xlm': (XLMWithLMHeadModel, XLMTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace()\n",
    "args.no_cuda = False\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "#Token at which text generation is stopped\n",
    "args.stop_token = None\n",
    "\n",
    "# XLM masked-language modeling (MLM) models need masked token (see details in sample_sequence)\n",
    "is_xlm_mlm = args.model_type in [\"xlm\"] and 'mlm' in args.model_name_or_path\n",
    "if is_xlm_mlm:\n",
    "    xlm_mask_token = tokenizer.mask_token_id\n",
    "else:\n",
    "    xlm_mask_token = None\n",
    "    \n",
    "xlm_lang = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT-2 http://jalammar.github.io/illustrated-gpt2/\n",
    "args.model_type = \"gpt2\"\n",
    "args.model_name_or_path = \"gpt2\"\n",
    "\n",
    "args.length = 20\n",
    "\n",
    "args.num_samples = 1\n",
    "\n",
    "#temperature of 0 implies greedy sampling\n",
    "args.temperature = 1.0\n",
    "\n",
    "#sample a word from the entire list of size top_k using the score as the probability of selecting that word\n",
    "args.top_k = 40\n",
    "\n",
    "args.top_p = 0.9\n",
    "\n",
    "#Primarily useful for CTRL model; in that case, use 1.2\n",
    "args.repetition_penalty = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CTRL (Salesforce)\n",
    "\n",
    "#Needs more GPU memory, so use CPU\n",
    "args.no_cuda = True\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "\n",
    "args.model_type = \"ctrl\"\n",
    "args.model_name_or_path = \"ctrl\"\n",
    "\n",
    "args.length = 20\n",
    "\n",
    "args.num_samples = 1\n",
    "\n",
    "#temperature of 0 implies greedy sampling\n",
    "args.temperature = 0\n",
    "\n",
    "#sample a word from the entire list of size top_k using the score as the probability of selecting that word\n",
    "args.top_k = 40\n",
    "\n",
    "args.top_p = 0.9\n",
    "\n",
    "#Primarily useful for CTRL model; in that case, use 1.2\n",
    "args.repetition_penalty = 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XLM (Multi-lingual)\n",
    "#Language code = 'EL' for Greek\n",
    "args.model_type = \"xlm\"\n",
    "args.model_name_or_path = \"xlm-mlm-100-1280\"\n",
    "\n",
    "args.length = 20\n",
    "\n",
    "args.num_samples = 1\n",
    "\n",
    "#temperature of 0 implies greedy sampling\n",
    "args.temperature = 1.0\n",
    "\n",
    "#sample a word from the entire list of size top_k using the score as the probability of selecting that word\n",
    "args.top_k = 40\n",
    "\n",
    "args.top_p = 0.9\n",
    "\n",
    "#Primarily useful for CTRL model; in that case, use 1.2\n",
    "args.repetition_penalty = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/08/2019 22:29:50 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-vocab.json not found in cache or force_download set to True, downloading to C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmp_b8g6_yx\n",
      "100%|████████████████████████████████████████████████████████████████████| 5715183/5715183 [00:02<00:00, 2406699.68B/s]\n",
      "12/08/2019 22:29:53 - INFO - transformers.file_utils -   copying C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmp_b8g6_yx to cache at C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\36cc0aaffa16aeaa0c6f7b21e58a10a9ab609ed4dbd2ff17423fc95690b4a8bf.50ff3a1ade6a729ff2500ee529c5c0d5630ef2025abc4d2a25e845aca60bac98\n",
      "12/08/2019 22:29:53 - INFO - transformers.file_utils -   creating metadata file for C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\36cc0aaffa16aeaa0c6f7b21e58a10a9ab609ed4dbd2ff17423fc95690b4a8bf.50ff3a1ade6a729ff2500ee529c5c0d5630ef2025abc4d2a25e845aca60bac98\n",
      "12/08/2019 22:29:53 - INFO - transformers.file_utils -   removing temp file C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmp_b8g6_yx\n",
      "12/08/2019 22:29:54 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-merges.txt not found in cache or force_download set to True, downloading to C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmpd0mvx7pn\n",
      "100%|████████████████████████████████████████████████████████████████████| 2973279/2973279 [00:01<00:00, 1601443.69B/s]\n",
      "12/08/2019 22:29:56 - INFO - transformers.file_utils -   copying C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmpd0mvx7pn to cache at C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\3e86a68a8775a1b921af60bcce492264e458bc21ca4239639ea8c253fb18ac53.7b94ff5bc85062952d4da8c1ef6755c3f7fd50b4ba33d63407770a920d1b4711\n",
      "12/08/2019 22:29:56 - INFO - transformers.file_utils -   creating metadata file for C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\3e86a68a8775a1b921af60bcce492264e458bc21ca4239639ea8c253fb18ac53.7b94ff5bc85062952d4da8c1ef6755c3f7fd50b4ba33d63407770a920d1b4711\n",
      "12/08/2019 22:29:56 - INFO - transformers.file_utils -   removing temp file C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmpd0mvx7pn\n",
      "12/08/2019 22:29:56 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-vocab.json from cache at C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\36cc0aaffa16aeaa0c6f7b21e58a10a9ab609ed4dbd2ff17423fc95690b4a8bf.50ff3a1ade6a729ff2500ee529c5c0d5630ef2025abc4d2a25e845aca60bac98\n",
      "12/08/2019 22:29:56 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-merges.txt from cache at C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\3e86a68a8775a1b921af60bcce492264e458bc21ca4239639ea8c253fb18ac53.7b94ff5bc85062952d4da8c1ef6755c3f7fd50b4ba33d63407770a920d1b4711\n",
      "12/08/2019 22:29:58 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-config.json not found in cache or force_download set to True, downloading to C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmpnqch_91z\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 41478/41478 [00:00<00:00, 223629.93B/s]\n",
      "12/08/2019 22:29:59 - INFO - transformers.file_utils -   copying C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmpnqch_91z to cache at C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\230d9a959764b99634b4ade26fa5ee35f1c7cd9dc3739ff3ee40f6c57cf57c38.5404c635b209ec542678e76c91606da60ca8bb13127d387a3280d56f61919efd\n",
      "12/08/2019 22:29:59 - INFO - transformers.file_utils -   creating metadata file for C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\230d9a959764b99634b4ade26fa5ee35f1c7cd9dc3739ff3ee40f6c57cf57c38.5404c635b209ec542678e76c91606da60ca8bb13127d387a3280d56f61919efd\n",
      "12/08/2019 22:29:59 - INFO - transformers.file_utils -   removing temp file C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmpnqch_91z\n",
      "12/08/2019 22:29:59 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-config.json from cache at C:\\Users\\s.lagousis.POBUCA\\.cache\\torch\\transformers\\230d9a959764b99634b4ade26fa5ee35f1c7cd9dc3739ff3ee40f6c57cf57c38.5404c635b209ec542678e76c91606da60ca8bb13127d387a3280d56f61919efd\n",
      "12/08/2019 22:29:59 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"accumulate_gradients\": 4,\n",
      "  \"ae_steps\": [],\n",
      "  \"amp\": 2,\n",
      "  \"asm\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"batch_size\": 16,\n",
      "  \"beam_size\": 1,\n",
      "  \"bos_index\": 0,\n",
      "  \"bptt\": 256,\n",
      "  \"bt_src_langs\": [],\n",
      "  \"bt_steps\": [],\n",
      "  \"causal\": false,\n",
      "  \"clip_grad_norm\": 1.0,\n",
      "  \"clm_steps\": [],\n",
      "  \"command\": \"python /private/home/aconneau/workdir/xlm_17_100_big.3/2019_08_10_19_23_42/train.py --n_heads 16 --bt_steps '' --max_vocab 200000 --word_mask_keep_rand '0.8,0.1,0.1' --use_lang_emb false --data_path '/private/home/aconneau/projects/XLM/data/wiki/100/175k' --save_periodic 0 --max_len 200 --bptt 256 --ae_steps '' --fp16 true --share_inout_emb true --sinusoidal_embeddings false --word_shuffle 0 --tokens_per_batch '-1' --accumulate_gradients 4 --validation_metrics '_valid_en_mlm_ppl,_valid_mlm_ppl,_valid_zh_mlm_ppl' --attention_dropout '0.1' --split_data true --max_epoch 100000 --stopping_criterion '_valid_zh_mlm_ppl,25' --dump_path '/checkpoint/aconneau/dumped' --epoch_size 200000 --word_blank 0 --gelu_activation true --n_layers 16 --optimizer 'adam_inverse_sqrt,lr=0.00005,warmup_updates=30000,beta1=0.9,beta2=0.999,weight_decay=0.01,eps=0.000001' --mlm_steps 'en,es,fr,de,zh,ru,pt,it,ar,ja,id,tr,nl,pl,simple,fa,vi,sv,ko,he,ro,no,hi,uk,cs,fi,hu,th,da,ca,el,bg,sr,ms,bn,hr,sl,zh_yue,az,sk,eo,ta,sh,lt,et,ml,la,bs,sq,arz,af,ka,mr,eu,tl,ang,gl,nn,ur,kk,be,hy,te,lv,mk,zh_classical,als,is,wuu,my,sco,mn,ceb,ast,cy,kn,br,an,gu,bar,uz,lb,ne,si,war,jv,ga,zh_min_nan,oc,ku,sw,nds,ckb,ia,yi,fy,scn,gan,tt,am' --eval_bleu false --dropout '0.1' --mt_steps '' --batch_size 16 --word_dropout 0 --reload_model '/checkpoint/aconneau/dumped/xlm_17_100_240_big_model_upper.2/14884511/best-valid_zh_mlm_ppl.pth' --min_count 0 --amp 2 --group_by_size true --asm false --sample_alpha '0.5' --word_pred '0.15' --clip_grad_norm 1 --emb_dim 1280 --encoder_only true --lgs 'en-es-fr-de-zh-ru-pt-it-ar-ja-id-tr-nl-pl-simple-fa-vi-sv-ko-he-ro-no-hi-uk-cs-fi-hu-th-da-ca-el-bg-sr-ms-bn-hr-sl-zh_yue-az-sk-eo-ta-sh-lt-et-ml-la-bs-sq-arz-af-ka-mr-eu-tl-ang-gl-nn-ur-kk-be-hy-te-lv-mk-zh_classical-als-is-wuu-my-sco-mn-ceb-ast-cy-kn-br-an-gu-bar-uz-lb-ne-si-war-jv-ga-zh_min_nan-oc-ku-sw-nds-ckb-ia-yi-fy-scn-gan-tt-am' --clm_steps '' --exp_name 'xlm_17_100_big.3' --lg_sampling_factor '0.7' --eval_only false --exp_id 16656234 --master_port 11363 --exp_id \\\"16656234\\\"\",\n",
      "  \"context_size\": 0,\n",
      "  \"data_path\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k\",\n",
      "  \"debug\": false,\n",
      "  \"debug_slurm\": false,\n",
      "  \"debug_train\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dump_path\": \"/checkpoint/aconneau/dumped/xlm_17_100_big.3/16656234\",\n",
      "  \"early_stopping\": false,\n",
      "  \"emb_dim\": 1280,\n",
      "  \"embed_init_std\": 0.02209708691207961,\n",
      "  \"encoder_only\": true,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_index\": 1,\n",
      "  \"epoch_size\": 200000,\n",
      "  \"eval_bleu\": false,\n",
      "  \"eval_only\": false,\n",
      "  \"exp_id\": \"16656234\",\n",
      "  \"exp_name\": \"xlm_17_100_big.3\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"fp16\": true,\n",
      "  \"gelu_activation\": true,\n",
      "  \"global_rank\": 0,\n",
      "  \"group_by_size\": true,\n",
      "  \"hyp_path\": \"/checkpoint/aconneau/dumped/xlm_17_100_big.3/16656234/hypotheses\",\n",
      "  \"id2lang\": {\n",
      "    \"0\": \"af\",\n",
      "    \"1\": \"als\",\n",
      "    \"10\": \"be\",\n",
      "    \"11\": \"bg\",\n",
      "    \"12\": \"bn\",\n",
      "    \"13\": \"br\",\n",
      "    \"14\": \"bs\",\n",
      "    \"15\": \"ca\",\n",
      "    \"16\": \"ceb\",\n",
      "    \"17\": \"ckb\",\n",
      "    \"18\": \"cs\",\n",
      "    \"19\": \"cy\",\n",
      "    \"2\": \"am\",\n",
      "    \"20\": \"da\",\n",
      "    \"21\": \"de\",\n",
      "    \"22\": \"el\",\n",
      "    \"23\": \"en\",\n",
      "    \"24\": \"eo\",\n",
      "    \"25\": \"es\",\n",
      "    \"26\": \"et\",\n",
      "    \"27\": \"eu\",\n",
      "    \"28\": \"fa\",\n",
      "    \"29\": \"fi\",\n",
      "    \"3\": \"an\",\n",
      "    \"30\": \"fr\",\n",
      "    \"31\": \"fy\",\n",
      "    \"32\": \"ga\",\n",
      "    \"33\": \"gan\",\n",
      "    \"34\": \"gl\",\n",
      "    \"35\": \"gu\",\n",
      "    \"36\": \"he\",\n",
      "    \"37\": \"hi\",\n",
      "    \"38\": \"hr\",\n",
      "    \"39\": \"hu\",\n",
      "    \"4\": \"ang\",\n",
      "    \"40\": \"hy\",\n",
      "    \"41\": \"ia\",\n",
      "    \"42\": \"id\",\n",
      "    \"43\": \"is\",\n",
      "    \"44\": \"it\",\n",
      "    \"45\": \"ja\",\n",
      "    \"46\": \"jv\",\n",
      "    \"47\": \"ka\",\n",
      "    \"48\": \"kk\",\n",
      "    \"49\": \"kn\",\n",
      "    \"5\": \"ar\",\n",
      "    \"50\": \"ko\",\n",
      "    \"51\": \"ku\",\n",
      "    \"52\": \"la\",\n",
      "    \"53\": \"lb\",\n",
      "    \"54\": \"lt\",\n",
      "    \"55\": \"lv\",\n",
      "    \"56\": \"mk\",\n",
      "    \"57\": \"ml\",\n",
      "    \"58\": \"mn\",\n",
      "    \"59\": \"mr\",\n",
      "    \"6\": \"arz\",\n",
      "    \"60\": \"ms\",\n",
      "    \"61\": \"my\",\n",
      "    \"62\": \"nds\",\n",
      "    \"63\": \"ne\",\n",
      "    \"64\": \"nl\",\n",
      "    \"65\": \"nn\",\n",
      "    \"66\": \"no\",\n",
      "    \"67\": \"oc\",\n",
      "    \"68\": \"pl\",\n",
      "    \"69\": \"pt\",\n",
      "    \"7\": \"ast\",\n",
      "    \"70\": \"ro\",\n",
      "    \"71\": \"ru\",\n",
      "    \"72\": \"scn\",\n",
      "    \"73\": \"sco\",\n",
      "    \"74\": \"sh\",\n",
      "    \"75\": \"si\",\n",
      "    \"76\": \"simple\",\n",
      "    \"77\": \"sk\",\n",
      "    \"78\": \"sl\",\n",
      "    \"79\": \"sq\",\n",
      "    \"8\": \"az\",\n",
      "    \"80\": \"sr\",\n",
      "    \"81\": \"sv\",\n",
      "    \"82\": \"sw\",\n",
      "    \"83\": \"ta\",\n",
      "    \"84\": \"te\",\n",
      "    \"85\": \"th\",\n",
      "    \"86\": \"tl\",\n",
      "    \"87\": \"tr\",\n",
      "    \"88\": \"tt\",\n",
      "    \"89\": \"uk\",\n",
      "    \"9\": \"bar\",\n",
      "    \"90\": \"ur\",\n",
      "    \"91\": \"uz\",\n",
      "    \"92\": \"vi\",\n",
      "    \"93\": \"war\",\n",
      "    \"94\": \"wuu\",\n",
      "    \"95\": \"yi\",\n",
      "    \"96\": \"zh\",\n",
      "    \"97\": \"zh_classical\",\n",
      "    \"98\": \"zh_min_nan\",\n",
      "    \"99\": \"zh_yue\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder\": true,\n",
      "  \"is_master\": true,\n",
      "  \"is_slurm_job\": true,\n",
      "  \"lambda_ae\": 1.0,\n",
      "  \"lambda_ae_config\": null,\n",
      "  \"lambda_bt\": 1.0,\n",
      "  \"lambda_bt_config\": null,\n",
      "  \"lambda_clm\": 1.0,\n",
      "  \"lambda_clm_config\": null,\n",
      "  \"lambda_mlm\": 1.0,\n",
      "  \"lambda_mlm_config\": null,\n",
      "  \"lambda_mt\": 1.0,\n",
      "  \"lambda_mt_config\": null,\n",
      "  \"lambda_pc\": 1.0,\n",
      "  \"lambda_pc_config\": null,\n",
      "  \"lang2id\": {\n",
      "    \"af\": 0,\n",
      "    \"als\": 1,\n",
      "    \"am\": 2,\n",
      "    \"an\": 3,\n",
      "    \"ang\": 4,\n",
      "    \"ar\": 5,\n",
      "    \"arz\": 6,\n",
      "    \"ast\": 7,\n",
      "    \"az\": 8,\n",
      "    \"bar\": 9,\n",
      "    \"be\": 10,\n",
      "    \"bg\": 11,\n",
      "    \"bn\": 12,\n",
      "    \"br\": 13,\n",
      "    \"bs\": 14,\n",
      "    \"ca\": 15,\n",
      "    \"ceb\": 16,\n",
      "    \"ckb\": 17,\n",
      "    \"cs\": 18,\n",
      "    \"cy\": 19,\n",
      "    \"da\": 20,\n",
      "    \"de\": 21,\n",
      "    \"el\": 22,\n",
      "    \"en\": 23,\n",
      "    \"eo\": 24,\n",
      "    \"es\": 25,\n",
      "    \"et\": 26,\n",
      "    \"eu\": 27,\n",
      "    \"fa\": 28,\n",
      "    \"fi\": 29,\n",
      "    \"fr\": 30,\n",
      "    \"fy\": 31,\n",
      "    \"ga\": 32,\n",
      "    \"gan\": 33,\n",
      "    \"gl\": 34,\n",
      "    \"gu\": 35,\n",
      "    \"he\": 36,\n",
      "    \"hi\": 37,\n",
      "    \"hr\": 38,\n",
      "    \"hu\": 39,\n",
      "    \"hy\": 40,\n",
      "    \"ia\": 41,\n",
      "    \"id\": 42,\n",
      "    \"is\": 43,\n",
      "    \"it\": 44,\n",
      "    \"ja\": 45,\n",
      "    \"jv\": 46,\n",
      "    \"ka\": 47,\n",
      "    \"kk\": 48,\n",
      "    \"kn\": 49,\n",
      "    \"ko\": 50,\n",
      "    \"ku\": 51,\n",
      "    \"la\": 52,\n",
      "    \"lb\": 53,\n",
      "    \"lt\": 54,\n",
      "    \"lv\": 55,\n",
      "    \"mk\": 56,\n",
      "    \"ml\": 57,\n",
      "    \"mn\": 58,\n",
      "    \"mr\": 59,\n",
      "    \"ms\": 60,\n",
      "    \"my\": 61,\n",
      "    \"nds\": 62,\n",
      "    \"ne\": 63,\n",
      "    \"nl\": 64,\n",
      "    \"nn\": 65,\n",
      "    \"no\": 66,\n",
      "    \"oc\": 67,\n",
      "    \"pl\": 68,\n",
      "    \"pt\": 69,\n",
      "    \"ro\": 70,\n",
      "    \"ru\": 71,\n",
      "    \"scn\": 72,\n",
      "    \"sco\": 73,\n",
      "    \"sh\": 74,\n",
      "    \"si\": 75,\n",
      "    \"simple\": 76,\n",
      "    \"sk\": 77,\n",
      "    \"sl\": 78,\n",
      "    \"sq\": 79,\n",
      "    \"sr\": 80,\n",
      "    \"sv\": 81,\n",
      "    \"sw\": 82,\n",
      "    \"ta\": 83,\n",
      "    \"te\": 84,\n",
      "    \"th\": 85,\n",
      "    \"tl\": 86,\n",
      "    \"tr\": 87,\n",
      "    \"tt\": 88,\n",
      "    \"uk\": 89,\n",
      "    \"ur\": 90,\n",
      "    \"uz\": 91,\n",
      "    \"vi\": 92,\n",
      "    \"war\": 93,\n",
      "    \"wuu\": 94,\n",
      "    \"yi\": 95,\n",
      "    \"zh\": 96,\n",
      "    \"zh_classical\": 97,\n",
      "    \"zh_min_nan\": 98,\n",
      "    \"zh_yue\": 99\n",
      "  },\n",
      "  \"langs\": [\n",
      "    \"en\",\n",
      "    \"es\",\n",
      "    \"fr\",\n",
      "    \"de\",\n",
      "    \"zh\",\n",
      "    \"ru\",\n",
      "    \"pt\",\n",
      "    \"it\",\n",
      "    \"ar\",\n",
      "    \"ja\",\n",
      "    \"id\",\n",
      "    \"tr\",\n",
      "    \"nl\",\n",
      "    \"pl\",\n",
      "    \"simple\",\n",
      "    \"fa\",\n",
      "    \"vi\",\n",
      "    \"sv\",\n",
      "    \"ko\",\n",
      "    \"he\",\n",
      "    \"ro\",\n",
      "    \"no\",\n",
      "    \"hi\",\n",
      "    \"uk\",\n",
      "    \"cs\",\n",
      "    \"fi\",\n",
      "    \"hu\",\n",
      "    \"th\",\n",
      "    \"da\",\n",
      "    \"ca\",\n",
      "    \"el\",\n",
      "    \"bg\",\n",
      "    \"sr\",\n",
      "    \"ms\",\n",
      "    \"bn\",\n",
      "    \"hr\",\n",
      "    \"sl\",\n",
      "    \"zh_yue\",\n",
      "    \"az\",\n",
      "    \"sk\",\n",
      "    \"eo\",\n",
      "    \"ta\",\n",
      "    \"sh\",\n",
      "    \"lt\",\n",
      "    \"et\",\n",
      "    \"ml\",\n",
      "    \"la\",\n",
      "    \"bs\",\n",
      "    \"sq\",\n",
      "    \"arz\",\n",
      "    \"af\",\n",
      "    \"ka\",\n",
      "    \"mr\",\n",
      "    \"eu\",\n",
      "    \"tl\",\n",
      "    \"ang\",\n",
      "    \"gl\",\n",
      "    \"nn\",\n",
      "    \"ur\",\n",
      "    \"kk\",\n",
      "    \"be\",\n",
      "    \"hy\",\n",
      "    \"te\",\n",
      "    \"lv\",\n",
      "    \"mk\",\n",
      "    \"zh_classical\",\n",
      "    \"als\",\n",
      "    \"is\",\n",
      "    \"wuu\",\n",
      "    \"my\",\n",
      "    \"sco\",\n",
      "    \"mn\",\n",
      "    \"ceb\",\n",
      "    \"ast\",\n",
      "    \"cy\",\n",
      "    \"kn\",\n",
      "    \"br\",\n",
      "    \"an\",\n",
      "    \"gu\",\n",
      "    \"bar\",\n",
      "    \"uz\",\n",
      "    \"lb\",\n",
      "    \"ne\",\n",
      "    \"si\",\n",
      "    \"war\",\n",
      "    \"jv\",\n",
      "    \"ga\",\n",
      "    \"zh_min_nan\",\n",
      "    \"oc\",\n",
      "    \"ku\",\n",
      "    \"sw\",\n",
      "    \"nds\",\n",
      "    \"ckb\",\n",
      "    \"ia\",\n",
      "    \"yi\",\n",
      "    \"fy\",\n",
      "    \"scn\",\n",
      "    \"gan\",\n",
      "    \"tt\",\n",
      "    \"am\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1,\n",
      "  \"lg_sampling_factor\": 0.7,\n",
      "  \"lgs\": \"en-es-fr-de-zh-ru-pt-it-ar-ja-id-tr-nl-pl-simple-fa-vi-sv-ko-he-ro-no-hi-uk-cs-fi-hu-th-da-ca-el-bg-sr-ms-bn-hr-sl-zh_yue-az-sk-eo-ta-sh-lt-et-ml-la-bs-sq-arz-af-ka-mr-eu-tl-ang-gl-nn-ur-kk-be-hy-te-lv-mk-zh_classical-als-is-wuu-my-sco-mn-ceb-ast-cy-kn-br-an-gu-bar-uz-lb-ne-si-war-jv-ga-zh_min_nan-oc-ku-sw-nds-ckb-ia-yi-fy-scn-gan-tt-am\",\n",
      "  \"local_rank\": 0,\n",
      "  \"mask_index\": 5,\n",
      "  \"master_addr\": \"learnfair0332\",\n",
      "  \"master_port\": 11363,\n",
      "  \"max_batch_size\": 0,\n",
      "  \"max_epoch\": 100000,\n",
      "  \"max_len\": 200,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_vocab\": 200000,\n",
      "  \"min_count\": 0,\n",
      "  \"mlm_steps\": [\n",
      "    [\n",
      "      \"en\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"es\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"fr\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"de\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"zh\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ru\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"pt\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"it\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ar\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ja\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"id\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"tr\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"nl\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"pl\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"simple\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"fa\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"vi\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sv\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ko\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"he\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ro\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"no\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"hi\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"uk\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"cs\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"fi\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"hu\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"th\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"da\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ca\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"el\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"bg\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sr\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ms\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"bn\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"hr\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sl\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"zh_yue\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"az\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sk\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"eo\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ta\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sh\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"lt\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"et\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ml\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"la\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"bs\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sq\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"arz\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"af\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ka\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"mr\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"eu\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"tl\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ang\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"gl\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"nn\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ur\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"kk\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"be\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"hy\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"te\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"lv\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"mk\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"zh_classical\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"als\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"is\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"wuu\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"my\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sco\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"mn\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ceb\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ast\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"cy\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"kn\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"br\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"an\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"gu\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"bar\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"uz\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"lb\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ne\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"si\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"war\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"jv\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ga\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"zh_min_nan\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"oc\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ku\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"sw\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"nds\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ckb\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"ia\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"yi\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"fy\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"scn\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"gan\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"tt\",\n",
      "      null\n",
      "    ],\n",
      "    [\n",
      "      \"am\",\n",
      "      null\n",
      "    ]\n",
      "  ],\n",
      "  \"mono_dataset\": {\n",
      "    \"af\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.af.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.af.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.af.pth\"\n",
      "    },\n",
      "    \"als\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.als.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.als.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.als.pth\"\n",
      "    },\n",
      "    \"am\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.am.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.am.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.am.pth\"\n",
      "    },\n",
      "    \"an\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.an.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.an.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.an.pth\"\n",
      "    },\n",
      "    \"ang\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ang.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ang.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ang.pth\"\n",
      "    },\n",
      "    \"ar\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ar.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ar.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ar.pth\"\n",
      "    },\n",
      "    \"arz\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.arz.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.arz.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.arz.pth\"\n",
      "    },\n",
      "    \"ast\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ast.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ast.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ast.pth\"\n",
      "    },\n",
      "    \"az\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.az.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.az.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.az.pth\"\n",
      "    },\n",
      "    \"bar\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.bar.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.bar.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.bar.pth\"\n",
      "    },\n",
      "    \"be\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.be.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.be.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.be.pth\"\n",
      "    },\n",
      "    \"bg\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.bg.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.bg.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.bg.pth\"\n",
      "    },\n",
      "    \"bn\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.bn.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.bn.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.bn.pth\"\n",
      "    },\n",
      "    \"br\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.br.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.br.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.br.pth\"\n",
      "    },\n",
      "    \"bs\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.bs.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.bs.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.bs.pth\"\n",
      "    },\n",
      "    \"ca\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ca.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ca.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ca.pth\"\n",
      "    },\n",
      "    \"ceb\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ceb.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ceb.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ceb.pth\"\n",
      "    },\n",
      "    \"ckb\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ckb.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ckb.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ckb.pth\"\n",
      "    },\n",
      "    \"cs\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.cs.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.cs.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.cs.pth\"\n",
      "    },\n",
      "    \"cy\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.cy.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.cy.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.cy.pth\"\n",
      "    },\n",
      "    \"da\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.da.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.da.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.da.pth\"\n",
      "    },\n",
      "    \"de\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.de.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.de.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.de.pth\"\n",
      "    },\n",
      "    \"el\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.el.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.el.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.el.pth\"\n",
      "    },\n",
      "    \"en\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.en.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.en.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.en.pth\"\n",
      "    },\n",
      "    \"eo\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.eo.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.eo.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.eo.pth\"\n",
      "    },\n",
      "    \"es\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.es.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.es.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.es.pth\"\n",
      "    },\n",
      "    \"et\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.et.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.et.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.et.pth\"\n",
      "    },\n",
      "    \"eu\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.eu.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.eu.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.eu.pth\"\n",
      "    },\n",
      "    \"fa\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.fa.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.fa.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.fa.pth\"\n",
      "    },\n",
      "    \"fi\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.fi.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.fi.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.fi.pth\"\n",
      "    },\n",
      "    \"fr\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.fr.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.fr.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.fr.pth\"\n",
      "    },\n",
      "    \"fy\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.fy.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.fy.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.fy.pth\"\n",
      "    },\n",
      "    \"ga\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ga.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ga.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ga.pth\"\n",
      "    },\n",
      "    \"gan\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.gan.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.gan.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.gan.pth\"\n",
      "    },\n",
      "    \"gl\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.gl.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.gl.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.gl.pth\"\n",
      "    },\n",
      "    \"gu\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.gu.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.gu.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.gu.pth\"\n",
      "    },\n",
      "    \"he\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.he.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.he.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.he.pth\"\n",
      "    },\n",
      "    \"hi\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.hi.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.hi.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.hi.pth\"\n",
      "    },\n",
      "    \"hr\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.hr.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.hr.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.hr.pth\"\n",
      "    },\n",
      "    \"hu\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.hu.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.hu.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.hu.pth\"\n",
      "    },\n",
      "    \"hy\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.hy.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.hy.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.hy.pth\"\n",
      "    },\n",
      "    \"ia\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ia.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ia.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ia.pth\"\n",
      "    },\n",
      "    \"id\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.id.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.id.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.id.pth\"\n",
      "    },\n",
      "    \"is\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.is.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.is.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.is.pth\"\n",
      "    },\n",
      "    \"it\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.it.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.it.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.it.pth\"\n",
      "    },\n",
      "    \"ja\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ja.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ja.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ja.pth\"\n",
      "    },\n",
      "    \"jv\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.jv.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.jv.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.jv.pth\"\n",
      "    },\n",
      "    \"ka\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ka.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ka.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ka.pth\"\n",
      "    },\n",
      "    \"kk\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.kk.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.kk.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.kk.pth\"\n",
      "    },\n",
      "    \"kn\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.kn.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.kn.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.kn.pth\"\n",
      "    },\n",
      "    \"ko\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ko.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ko.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ko.pth\"\n",
      "    },\n",
      "    \"ku\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ku.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ku.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ku.pth\"\n",
      "    },\n",
      "    \"la\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.la.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.la.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.la.pth\"\n",
      "    },\n",
      "    \"lb\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.lb.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.lb.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.lb.pth\"\n",
      "    },\n",
      "    \"lt\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.lt.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.lt.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.lt.pth\"\n",
      "    },\n",
      "    \"lv\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.lv.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.lv.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.lv.pth\"\n",
      "    },\n",
      "    \"mk\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.mk.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.mk.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.mk.pth\"\n",
      "    },\n",
      "    \"ml\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ml.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ml.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ml.pth\"\n",
      "    },\n",
      "    \"mn\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.mn.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.mn.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.mn.pth\"\n",
      "    },\n",
      "    \"mr\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.mr.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.mr.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.mr.pth\"\n",
      "    },\n",
      "    \"ms\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ms.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ms.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ms.pth\"\n",
      "    },\n",
      "    \"my\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.my.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.my.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.my.pth\"\n",
      "    },\n",
      "    \"nds\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.nds.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.nds.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.nds.pth\"\n",
      "    },\n",
      "    \"ne\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ne.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ne.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ne.pth\"\n",
      "    },\n",
      "    \"nl\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.nl.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.nl.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.nl.pth\"\n",
      "    },\n",
      "    \"nn\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.nn.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.nn.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.nn.pth\"\n",
      "    },\n",
      "    \"no\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.no.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.no.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.no.pth\"\n",
      "    },\n",
      "    \"oc\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.oc.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.oc.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.oc.pth\"\n",
      "    },\n",
      "    \"pl\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.pl.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.pl.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.pl.pth\"\n",
      "    },\n",
      "    \"pt\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.pt.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.pt.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.pt.pth\"\n",
      "    },\n",
      "    \"ro\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ro.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ro.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ro.pth\"\n",
      "    },\n",
      "    \"ru\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ru.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ru.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ru.pth\"\n",
      "    },\n",
      "    \"scn\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.scn.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.scn.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.scn.pth\"\n",
      "    },\n",
      "    \"sco\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sco.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sco.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sco.pth\"\n",
      "    },\n",
      "    \"sh\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sh.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sh.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sh.pth\"\n",
      "    },\n",
      "    \"si\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.si.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.si.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.si.pth\"\n",
      "    },\n",
      "    \"simple\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.simple.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.simple.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.simple.pth\"\n",
      "    },\n",
      "    \"sk\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sk.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sk.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sk.pth\"\n",
      "    },\n",
      "    \"sl\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sl.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sl.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sl.pth\"\n",
      "    },\n",
      "    \"sq\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sq.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sq.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sq.pth\"\n",
      "    },\n",
      "    \"sr\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sr.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sr.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sr.pth\"\n",
      "    },\n",
      "    \"sv\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sv.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sv.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sv.pth\"\n",
      "    },\n",
      "    \"sw\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.sw.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.sw.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.sw.pth\"\n",
      "    },\n",
      "    \"ta\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ta.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ta.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ta.pth\"\n",
      "    },\n",
      "    \"te\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.te.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.te.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.te.pth\"\n",
      "    },\n",
      "    \"th\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.th.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.th.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.th.pth\"\n",
      "    },\n",
      "    \"tl\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.tl.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.tl.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.tl.pth\"\n",
      "    },\n",
      "    \"tr\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.tr.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.tr.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.tr.pth\"\n",
      "    },\n",
      "    \"tt\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.tt.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.tt.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.tt.pth\"\n",
      "    },\n",
      "    \"uk\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.uk.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.uk.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.uk.pth\"\n",
      "    },\n",
      "    \"ur\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.ur.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.ur.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.ur.pth\"\n",
      "    },\n",
      "    \"uz\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.uz.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.uz.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.uz.pth\"\n",
      "    },\n",
      "    \"vi\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.vi.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.vi.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.vi.pth\"\n",
      "    },\n",
      "    \"war\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.war.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.war.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.war.pth\"\n",
      "    },\n",
      "    \"wuu\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.wuu.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.wuu.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.wuu.pth\"\n",
      "    },\n",
      "    \"yi\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.yi.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.yi.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.yi.pth\"\n",
      "    },\n",
      "    \"zh\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.zh.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.zh.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.zh.pth\"\n",
      "    },\n",
      "    \"zh_classical\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.zh_classical.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.zh_classical.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.zh_classical.pth\"\n",
      "    },\n",
      "    \"zh_min_nan\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.zh_min_nan.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.zh_min_nan.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.zh_min_nan.pth\"\n",
      "    },\n",
      "    \"zh_yue\": {\n",
      "      \"test\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/test.zh_yue.pth\",\n",
      "      \"train\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/train.zh_yue.pth\",\n",
      "      \"valid\": \"/private/home/aconneau/projects/XLM/data/wiki/100/175k/valid.zh_yue.pth\"\n",
      "    }\n",
      "  },\n",
      "  \"mono_list\": [\n",
      "    \"en\",\n",
      "    \"es\",\n",
      "    \"fr\",\n",
      "    \"de\",\n",
      "    \"zh\",\n",
      "    \"ru\",\n",
      "    \"pt\",\n",
      "    \"it\",\n",
      "    \"ar\",\n",
      "    \"ja\",\n",
      "    \"id\",\n",
      "    \"tr\",\n",
      "    \"nl\",\n",
      "    \"pl\",\n",
      "    \"simple\",\n",
      "    \"fa\",\n",
      "    \"vi\",\n",
      "    \"sv\",\n",
      "    \"ko\",\n",
      "    \"he\",\n",
      "    \"ro\",\n",
      "    \"no\",\n",
      "    \"hi\",\n",
      "    \"uk\",\n",
      "    \"cs\",\n",
      "    \"fi\",\n",
      "    \"hu\",\n",
      "    \"th\",\n",
      "    \"da\",\n",
      "    \"ca\",\n",
      "    \"el\",\n",
      "    \"bg\",\n",
      "    \"sr\",\n",
      "    \"ms\",\n",
      "    \"bn\",\n",
      "    \"hr\",\n",
      "    \"sl\",\n",
      "    \"zh_yue\",\n",
      "    \"az\",\n",
      "    \"sk\",\n",
      "    \"eo\",\n",
      "    \"ta\",\n",
      "    \"sh\",\n",
      "    \"lt\",\n",
      "    \"et\",\n",
      "    \"ml\",\n",
      "    \"la\",\n",
      "    \"bs\",\n",
      "    \"sq\",\n",
      "    \"arz\",\n",
      "    \"af\",\n",
      "    \"ka\",\n",
      "    \"mr\",\n",
      "    \"eu\",\n",
      "    \"tl\",\n",
      "    \"ang\",\n",
      "    \"gl\",\n",
      "    \"nn\",\n",
      "    \"ur\",\n",
      "    \"kk\",\n",
      "    \"be\",\n",
      "    \"hy\",\n",
      "    \"te\",\n",
      "    \"lv\",\n",
      "    \"mk\",\n",
      "    \"zh_classical\",\n",
      "    \"als\",\n",
      "    \"is\",\n",
      "    \"wuu\",\n",
      "    \"my\",\n",
      "    \"sco\",\n",
      "    \"mn\",\n",
      "    \"ceb\",\n",
      "    \"ast\",\n",
      "    \"cy\",\n",
      "    \"kn\",\n",
      "    \"br\",\n",
      "    \"an\",\n",
      "    \"gu\",\n",
      "    \"bar\",\n",
      "    \"uz\",\n",
      "    \"lb\",\n",
      "    \"ne\",\n",
      "    \"si\",\n",
      "    \"war\",\n",
      "    \"jv\",\n",
      "    \"ga\",\n",
      "    \"zh_min_nan\",\n",
      "    \"oc\",\n",
      "    \"ku\",\n",
      "    \"sw\",\n",
      "    \"nds\",\n",
      "    \"ckb\",\n",
      "    \"ia\",\n",
      "    \"yi\",\n",
      "    \"fy\",\n",
      "    \"scn\",\n",
      "    \"gan\",\n",
      "    \"tt\",\n",
      "    \"am\"\n",
      "  ],\n",
      "  \"mt_steps\": [],\n",
      "  \"multi_gpu\": true,\n",
      "  \"multi_node\": true,\n",
      "  \"n_gpu_per_node\": 8,\n",
      "  \"n_heads\": 16,\n",
      "  \"n_langs\": 100,\n",
      "  \"n_layers\": 16,\n",
      "  \"n_nodes\": 4,\n",
      "  \"n_words\": 200000,\n",
      "  \"node_id\": 0,\n",
      "  \"num_labels\": 2,\n",
      "  \"optimizer\": \"adam_inverse_sqrt,lr=0.00005,warmup_updates=30000,beta1=0.9,beta2=0.999,weight_decay=0.01,eps=0.000001\",\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_index\": 2,\n",
      "  \"para_dataset\": {},\n",
      "  \"para_list\": [],\n",
      "  \"pc_steps\": [],\n",
      "  \"pruned_heads\": {},\n",
      "  \"ref_paths\": {},\n",
      "  \"reload_checkpoint\": \"\",\n",
      "  \"reload_emb\": \"\",\n",
      "  \"reload_model\": \"/checkpoint/aconneau/dumped/xlm_17_100_240_big_model_upper.2/14884511/best-valid_zh_mlm_ppl.pth\",\n",
      "  \"sample_alpha\": 0.5,\n",
      "  \"save_periodic\": 0,\n",
      "  \"share_inout_emb\": true,\n",
      "  \"sinusoidal_embeddings\": false,\n",
      "  \"split_data\": true,\n",
      "  \"start_n_top\": 5,\n",
      "  \"stopping_criterion\": \"_valid_zh_mlm_ppl,25\",\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"tokens_per_batch\": -1,\n",
      "  \"torchscript\": false,\n",
      "  \"unk_index\": 3,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"use_lang_emb\": false,\n",
      "  \"use_memory\": false,\n",
      "  \"validation_metrics\": \"_valid_en_mlm_ppl,_valid_mlm_ppl,_valid_zh_mlm_ppl\",\n",
      "  \"word_blank\": 0.0,\n",
      "  \"word_dropout\": 0.0,\n",
      "  \"word_keep\": 0.1,\n",
      "  \"word_mask\": 0.8,\n",
      "  \"word_mask_keep_rand\": \"0.8,0.1,0.1\",\n",
      "  \"word_pred\": 0.15,\n",
      "  \"word_rand\": 0.1,\n",
      "  \"word_shuffle\": 0.0,\n",
      "  \"world_size\": 32\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/08/2019 22:30:00 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\SLAGOU~1.POB\\AppData\\Local\\Temp\\tmpg0m2dgcu\n",
      " 25%|███████████████▌                                               | 283402240/1143436263 [17:01<295:28:13, 808.54B/s]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-pytorch_model.bin' to download pretrained weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSysCallError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1821\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1822\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1638\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0merrno\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mSysCallError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrorcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1640\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSysCallError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Unexpected EOF\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSysCallError\u001b[0m: (10054, 'WSAECONNRESET')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m                 \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZeroReturnError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: (10054, 'WSAECONNRESET')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    458\u001b[0m                         \u001b[1;31m# Content-Length are caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength_remaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    377\u001b[0m                 \u001b[1;31m# This includes IncompleteRead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Connection broken: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: OSError(\"(10054, \\'WSAECONNRESET\\')\")', OSError(\"(10054, 'WSAECONNRESET')\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m                 resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir, force_download=force_download,\n\u001b[1;32m--> 374\u001b[1;33m                                                     proxies=proxies, resume_download=resume_download)\n\u001b[0m\u001b[0;32m    375\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m             resume_download=resume_download)\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download)\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                 \u001b[0mhttp_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# filter out keep-alive new chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: OSError(\"(10054, \\'WSAECONNRESET\\')\")', OSError(\"(10054, 'WSAECONNRESET')\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0928a89f9e26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMODEL_CLASSES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    385\u001b[0m                             \u001b[0marchive_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                             [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME])\n\u001b[1;32m--> 387\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-pytorch_model.bin' to download pretrained weights."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████▌                                               | 283417600/1143436263 [17:12<295:27:54, 808.54B/s]"
     ]
    }
   ],
   "source": [
    "args.model_type = args.model_type.lower()\n",
    "model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
    "model = model_class.from_pretrained(args.model_name_or_path)\n",
    "model.to(args.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTRLLMHeadModel(\n",
       "  (transformer): CTRLModel(\n",
       "    (w): Embedding(246534, 1280)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (h): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (6): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (7): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (8): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (9): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (10): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (11): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (12): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (13): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (14): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (15): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (16): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (17): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (18): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (19): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (20): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (21): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (22): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (23): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (24): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (25): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (26): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (27): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (28): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (29): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (30): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (31): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (32): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (33): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (34): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (35): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (36): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (37): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (38): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (39): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (40): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (41): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (42): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (43): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (44): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (45): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (46): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "      (47): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1)\n",
       "        (dropout2): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm(torch.Size([1280]), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=246534, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, source=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    is_xlnet=False, is_xlm_mlm=False, xlm_mask_token=None, xlm_lang=None, device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "            if is_xlnet: \n",
    "                # XLNet is a direct (predict same token, not next token) and bi-directional model by default\n",
    "                # => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)\n",
    "                input_ids = torch.cat((generated, torch.zeros((1, 1), dtype=torch.long, device=device)), dim=1)\n",
    "                perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float, device=device)\n",
    "                perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
    "                target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)\n",
    "                target_mapping[0, 0, -1] = 1.0  # predict last token\n",
    "                inputs = {'input_ids': input_ids, 'perm_mask': perm_mask, 'target_mapping': target_mapping}\n",
    "\n",
    "            if is_xlm_mlm and xlm_mask_token:\n",
    "                # XLM MLM models are direct models (predict same token, not next token)\n",
    "                # => need one additional dummy token in the input (will be masked and guessed)\n",
    "                input_ids = torch.cat((generated, torch.full((1, 1), xlm_mask_token, dtype=torch.long, device=device)), dim=1)\n",
    "                inputs = {'input_ids': input_ids}\n",
    "\n",
    "            if xlm_lang is not None:\n",
    "                inputs[\"langs\"] = torch.tensor([xlm_lang] * inputs[\"input_ids\"].shape[1], device=device).view(1, -1)\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:27<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation English : This is a natural language processing model that aims to generate coherent text in a controllable manner. ; German : >>\n",
      "Es handelt sich um ein naturwissenschaftliches Verfahren, das auf eine kohärente und\n"
     ]
    }
   ],
   "source": [
    "raw_text = 'Links I saw her staring at me'\n",
    "\n",
    "#raw_text = 'Translation English : This is a natural language processing model that aims to generate coherent text in a controllable manner. ; French :'\n",
    "raw_text = 'Translation English : This is a natural language processing model that aims to generate coherent text in a controllable manner. ; German :'\n",
    "\n",
    "context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "\n",
    "#print(context_tokens)\n",
    "#print(tokenizer.convert_ids_to_tokens(context_tokens))\n",
    "\n",
    "out = sample_sequence(\n",
    "     model=model,\n",
    "     context=context_tokens,\n",
    "     num_samples=args.num_samples,\n",
    "     length=args.length,\n",
    "     temperature=args.temperature,\n",
    "     top_k=args.top_k,\n",
    "     top_p=args.top_p,\n",
    "     repetition_penalty=args.repetition_penalty,\n",
    "     is_xlnet=bool(args.model_type == \"xlnet\"),\n",
    "     is_xlm_mlm=is_xlm_mlm,\n",
    "     xlm_mask_token=xlm_mask_token,\n",
    "     xlm_lang=xlm_lang,\n",
    "     device=args.device,\n",
    "     )\n",
    "\n",
    "out = out[:, len(context_tokens):].tolist()\n",
    "\n",
    "print(raw_text, '>>')\n",
    "for o in out:\n",
    "    text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
